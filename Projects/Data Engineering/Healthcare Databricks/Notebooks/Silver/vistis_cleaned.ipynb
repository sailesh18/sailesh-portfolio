{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8c6353-b634-4755-a6a2-514063e523b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fea01b7-8335-4c9c-ba0c-db2915eb380b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = create_logger(notebook_name=\"landing_to_bronze\", log_level=\"DEBUG\")\n",
    "logger.info(\"üöÄ Initializing landing_to_bronze notebook\")\n",
    "\n",
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "bronze_schema = pipeline_config[\"schemas\"][\"bronze\"]\n",
    "bronze_path = pipeline_config[\"paths\"][\"bronze_path\"]\n",
    "bronze_volume_path = pipeline_config[\"paths\"][\"bronze_volume_path\"]\n",
    "silver_schema = pipeline_config[\"schemas\"][\"silver\"]\n",
    "silver_path = pipeline_config[\"paths\"][\"silver_path\"]\n",
    "landing_schema= pipeline_config[\"schemas\"][\"landing\"]\n",
    "landing_path = pipeline_config[\"paths\"][\"landing_path\"]\n",
    "logs_schema = pipeline_config[\"schemas\"][\"logs\"]\n",
    "table_name = \"visits\"\n",
    "logger.info(\"Extracted frequently used config values into variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de61d66-e03d-4937-81ea-0bf81eafde29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver_errors\")\n",
    "\n",
    "bronze_input_path = f\"{bronze_volume_path}/{table_name}\"  # e.g. \"visits\"\n",
    "silver_output_table = f\"{catalog}.{silver_schema}.{table_name}\"\n",
    "error_output_table = f\"{catalog}.silver_errors.{table_name}_errors\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a90fa1-3369-4fca-9a56-5a7f869430b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# visits_to_silver.py (SCD Type 2 - Final for Visits Table)\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, trim, upper, lower, to_date, monotonically_increasing_id, when\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Setup ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver_errors\")\n",
    "\n",
    "# Paths and targets\n",
    "bronze_input_path = f\"{bronze_volume_path}/{table_name}\"\n",
    "silver_output_table = f\"{catalog}.{silver_schema}.{table_name}\"\n",
    "error_output_table = f\"{catalog}.silver_errors.{table_name}_errors\"\n",
    "\n",
    "# Read CSV\n",
    "def read_bronze_csv(path):\n",
    "    logger.info(f\"üì• Reading from Bronze: {path}\")\n",
    "    return spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "# Rename Columns\n",
    "def apply_column_mapping(df):\n",
    "    logger.info(\"üîÄ Applying column mappings\")\n",
    "    for old_col, new_col in column_mappings[table_name].items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "# Data Type Enforcement\n",
    "def enforce_data_types(df):\n",
    "    logger.info(\"üß™ Enforcing data types\")\n",
    "    for col_name, dtype in table_config[table_name][\"columns\"].items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(dtype))\n",
    "    return df\n",
    "\n",
    "# Standardize follow_up + visit_date\n",
    "def standardize_values(df):\n",
    "    logger.info(\"‚ôªÔ∏è Standardizing follow_up and visit_date values\")\n",
    "    if \"follow_up\" in df.columns:\n",
    "        df = df.withColumn(\"follow_up\",\n",
    "            when(lower(trim(col(\"follow_up\"))).isin(\"y\", \"yes\"), \"Yes\")\n",
    "           .when(lower(trim(col(\"follow_up\"))).isin(\"n\", \"no\"), \"No\")\n",
    "           .when(lower(trim(col(\"follow_up\"))) == \"pending\", \"Pending\")\n",
    "           .otherwise(col(\"follow_up\"))\n",
    "        )\n",
    "    if \"visit_date\" in df.columns:\n",
    "        df = df.withColumn(\"visit_date\", to_date(col(\"visit_date\"), \"dd-MM-yyyy\"))\n",
    "    return df\n",
    "\n",
    "# DQ Checks\n",
    "def run_dq_checks(df):\n",
    "    logger.info(\"‚úÖ Running DQ checks for 'visits'\")\n",
    "    dq_errors = None\n",
    "\n",
    "    def append_dq(df_err, new_rows, reason):\n",
    "        new_rows = new_rows.withColumn(\"dq_error\", lit(reason))\n",
    "        return new_rows if df_err is None else df_err.unionByName(new_rows)\n",
    "\n",
    "    if \"visit_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"visit_id\").isNull()), \"visit_id is null\")\n",
    "\n",
    "    if \"patient_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"patient_id\").isNull()), \"patient_id is null\")\n",
    "\n",
    "    if \"doctor_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"doctor_id\").isNull()), \"doctor_id is null\")\n",
    "\n",
    "    if \"visit_date\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"visit_date\").isNull()), \"visit_date is null\")\n",
    "\n",
    "    return dq_errors\n",
    "\n",
    "# Filter & Save Errors\n",
    "def filter_and_store_errors(df, dq_errors):\n",
    "    logger.info(\"üóëÔ∏è Filtering bad records\")\n",
    "    valid_df = df\n",
    "\n",
    "    if dq_errors is not None:\n",
    "        dq_errors = dq_errors.withColumn(\"dq_error_id\", monotonically_increasing_id())\n",
    "        dq_errors = dq_errors.withColumn(\"error_ts\", current_timestamp())\n",
    "        dq_errors.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(error_output_table)\n",
    "        logger.info(f\"‚ö†Ô∏è {dq_errors.count()} DQ errors written to: {error_output_table}\")\n",
    "\n",
    "        valid_df = df.join(dq_errors.select(\"visit_id\").distinct(), on=\"visit_id\", how=\"left_anti\")\n",
    "\n",
    "    return valid_df\n",
    "\n",
    "# SCD Type 2\n",
    "def apply_scd_type2(df):\n",
    "    logger.info(f\"üîÅ Applying SCD Type 2 to: {silver_output_table}\")\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_output_table):\n",
    "        df = df.withColumn(\"valid_from\", current_timestamp()) \\\n",
    "               .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "               .withColumn(\"is_current\", lit(True))\n",
    "        df.write.format(\"delta\").option(\"mergeschema\", \"true\").saveAsTable(silver_output_table)\n",
    "        logger.info(f\"‚úÖ Created new SCD Type 2 table: {silver_output_table}\")\n",
    "        return\n",
    "\n",
    "    current_df = spark.table(silver_output_table).filter(\"is_current = true\")\n",
    "    join_keys = [\"visit_id\"]\n",
    "    compare_cols = [c for c in df.columns if c not in join_keys + [\"valid_from\", \"valid_to\", \"is_current\", \"ingestion_date\"]]\n",
    "\n",
    "    source_df = df.alias(\"source\")\n",
    "    target_df = current_df.alias(\"target\")\n",
    "\n",
    "    join_cond = \" AND \".join([f\"source.{k} = target.{k}\" for k in join_keys])\n",
    "    change_expr = \" OR \".join([f\"target.{c} IS DISTINCT FROM source.{c}\" for c in compare_cols])\n",
    "\n",
    "    joined_df = source_df.join(target_df, on=join_keys, how=\"left\")\n",
    "    changed_df = joined_df.filter(f\"{change_expr} OR target.{join_keys[0]} IS NULL\").select(\"source.*\")\n",
    "\n",
    "    logger.info(f\"üìå Changed/new rows: {changed_df.count()}\")\n",
    "\n",
    "    if changed_df.count() == 0:\n",
    "        logger.info(\"‚úÖ No changes found. Skipping SCD2 merge.\")\n",
    "        return\n",
    "\n",
    "    changed_df = changed_df \\\n",
    "        .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, silver_output_table)\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        source=changed_df.alias(\"source\"),\n",
    "        condition=join_cond + \" AND target.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=change_expr,\n",
    "        set={\n",
    "            \"valid_to\": \"current_timestamp()\",\n",
    "            \"is_current\": \"false\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    changed_df = changed_df.select(\n",
    "        \"visit_id\",\n",
    "        \"patient_id\",\n",
    "        \"doctor_id\",\n",
    "        \"deartment\",\n",
    "        \"visit_date\",\n",
    "        \"follow_up\",\n",
    "        \"ingestion_date\",\n",
    "        \"valid_from\",\n",
    "        \"valid_to\",\n",
    "        \"is_current\"\n",
    "    )\n",
    "    changed_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(silver_output_table)\n",
    "    logger.info(\"‚úÖ SCD2 merge completed: history updated and new rows inserted.\")\n",
    "\n",
    "# Pipeline\n",
    "bronze_df = read_bronze_csv(bronze_input_path)\n",
    "bronze_df = apply_column_mapping(bronze_df)\n",
    "bronze_df = standardize_values(bronze_df)\n",
    "bronze_df = enforce_data_types(bronze_df)\n",
    "if \"ingestion_date\" not in bronze_df.columns:\n",
    "    bronze_df = bronze_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "dq_errors = run_dq_checks(bronze_df)\n",
    "clean_df = filter_and_store_errors(bronze_df, dq_errors)\n",
    "\n",
    "apply_scd_type2(clean_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8401636407211387,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "vistis_cleaned",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
