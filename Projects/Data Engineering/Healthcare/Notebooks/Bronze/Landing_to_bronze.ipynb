{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "418a2a8a-1d90-48b3-ae7a-271e936d5348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cf6791-ea95-4b94-baf6-c5f8e5abdb6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = create_logger(notebook_name=\"landing_to_bronze\", log_level=\"DEBUG\")\n",
    "logger.info(\"ðŸš€ Initializing landing_to_bronze notebook\")\n",
    "\n",
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "bronze_schema = pipeline_config[\"schemas\"][\"bronze\"]\n",
    "bronze_path = pipeline_config[\"paths\"][\"bronze_path\"]\n",
    "landing_schema= pipeline_config[\"schemas\"][\"landing\"]\n",
    "landing_path = pipeline_config[\"paths\"][\"landing_path\"]\n",
    "today_str = datetime.today().strftime('%Y%m%d')\n",
    "volume_name = f\"bronze_{today_str}\"\n",
    "volume_path = f\"/Volumes/{catalog}/{bronze_schema}/{volume_name}\"\n",
    "logs_schema = pipeline_config[\"schemas\"][\"logs\"]\n",
    "logger.info(\"Extracted frequently used config values into variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "72b9254f-f65f-4dcd-82d6-1475950fde97",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752087206900}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest data from landing volume\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "for file in tables_to_process:\n",
    "    # Read CSV file from the landing volume\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "        f\"{landing_path}/{file}.csv\"\n",
    "    ).withColumn(\"ingestion_date\", current_timestamp())\n",
    "    \n",
    "    # Display the DataFrame to verify the data\n",
    "    display(df)\n",
    "    \n",
    "    # Write the data into the bronze schema\n",
    "    df.write.format(\"delta\").option(\"mergeschema\",\"true\").mode(\"overwrite\").saveAsTable(\n",
    "        f\"{catalog}.{bronze_schema}.{file}\"\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Data ingested from {file} into {bronze_schema}.{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373d4804-279b-4749-ab51-932b6f12d784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "today_str = datetime.today().strftime('%Y%m%d')\n",
    "volume_name = f\"bronze_{today_str}\"\n",
    "volume_path = f\"/Volumes/{catalog}/{bronze_schema}/{volume_name}\"\n",
    "# === Create Volume ===\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS {catalog}.{bronze_schema}.{volume_name}\n",
    "\"\"\")\n",
    "\n",
    "# === Loop through each file and ingest ===\n",
    "for file in tables_to_process:\n",
    "    input_path = f\"{landing_path}/{file}.csv\"\n",
    "    \n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(input_path)\n",
    "        .withColumn(\"ingestion_date\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # Write to Delta inside the volume (as a folder for each table)\n",
    "    df.coalesce(1).write.format(\"csv\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{volume_path}/{file}.csv\")\n",
    "\n",
    "    logger.info(f\"âœ… Ingested {file}.csv into volume {volume_name} under folder {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "643ad13d-c595-4293-bb51-cf021531dd45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7756161104731408,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Landing_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
