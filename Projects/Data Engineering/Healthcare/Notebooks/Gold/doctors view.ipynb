{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f888b8fa-8b43-4963-abf0-db85024cebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8233ed-00d4-4eaf-bd4a-dd72c6442f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = create_logger(notebook_name=\"landing_to_bronze\", log_level=\"DEBUG\")\n",
    "logger.info(\"üöÄ Initializing landing_to_bronze notebook\")\n",
    "\n",
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "bronze_schema = pipeline_config[\"schemas\"][\"bronze\"]\n",
    "bronze_path = pipeline_config[\"paths\"][\"bronze_path\"]\n",
    "bronze_volume_path = pipeline_config[\"paths\"][\"bronze_volume_path\"]\n",
    "silver_schema = pipeline_config[\"schemas\"][\"silver\"]\n",
    "silver_path = pipeline_config[\"paths\"][\"silver_path\"]\n",
    "landing_schema= pipeline_config[\"schemas\"][\"landing\"]\n",
    "landing_path = pipeline_config[\"paths\"][\"landing_path\"]\n",
    "logs_schema = pipeline_config[\"schemas\"][\"logs\"]\n",
    "table_name = \"dim_doctors\"\n",
    "logger.info(\"Extracted frequently used config values into variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9faf9c1-52ed-49d1-a773-059a41e4c908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# doctor_performance.py (Gold View Builder - Doctor Performance)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ------------------------- Logging Start -------------------------\n",
    "logger.info(\"üöÄ Starting Gold View creation: vw_doctor_performance\")\n",
    "\n",
    "# ------------------------- Required Tables -------------------------\n",
    "required_tables = [\n",
    "    \"healthcare.gold.dim_doctors\",\n",
    "    \"healthcare.gold.dim_visits\",\n",
    "    \"healthcare.gold.fact_treatments\"\n",
    "]\n",
    "\n",
    "# ------------------------- Error Logging Function -------------------------\n",
    "def log_etl_error(entity_name, failure_stage, error_reason):\n",
    "    logger.error(f\"‚ùå Logging ETL error: [{entity_name}] - Stage: {failure_stage} - Reason: {error_reason}\")\n",
    "    \n",
    "    # Create DataFrame with error details\n",
    "    error_data = [(entity_name, failure_stage, error_reason)]\n",
    "    error_df = spark.createDataFrame(error_data, [\"entity_name\", \"failure_stage\", \"error_reason\"]) \\\n",
    "                    .withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "    # Check if error table exists; if not, create it\n",
    "    if not spark.catalog.tableExists(\"healthcare.gold.etl_error_log\"):\n",
    "        logger.warn(\"‚ö†Ô∏è Error log table not found, creating it now: healthcare.gold.etl_error_log\")\n",
    "        error_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"healthcare.gold.etl_error_log\")\n",
    "    else:\n",
    "        error_df.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .saveAsTable(\"healthcare.gold.etl_error_log\")\n",
    "\n",
    "# ------------------------- Table Existence Check -------------------------\n",
    "logger.info(\"üß™ Checking if required Gold tables exist\")\n",
    "missing_tables = [tbl for tbl in required_tables if not spark.catalog.tableExists(tbl)]\n",
    "\n",
    "if missing_tables:\n",
    "    reason = \", \".join(missing_tables)\n",
    "    log_etl_error(\n",
    "        entity_name=\"view_doctor_performance\",\n",
    "        failure_stage=\"check_tables\",\n",
    "        error_reason=reason\n",
    "    )\n",
    "    logger.info(\"üö® Aborting view creation due to missing input tables\")\n",
    "else:\n",
    "    logger.info(\"‚úÖ All required input tables are present\")\n",
    "\n",
    "    # ------------------------- SQL for View -------------------------\n",
    "    logger.info(\"üß† Constructing SQL for doctor performance view\")\n",
    "\n",
    "    view_sql = \"\"\"\n",
    "    CREATE OR REPLACE VIEW healthcare.gold.view_doctor_performance AS\n",
    "    SELECT \n",
    "        d.doctor_id,\n",
    "        d.doctor_name AS doctor_name,\n",
    "        d.department AS specialization,\n",
    "\n",
    "        COUNT(DISTINCT v.visit_id) AS total_visits,\n",
    "        COUNT(DISTINCT CASE WHEN v.follow_up = 'yes' THEN v.visit_id END) AS follow_up_visits,\n",
    "        ROUND(\n",
    "            COUNT(DISTINCT CASE WHEN v.follow_up = 'yes' THEN v.visit_id END) * 1.0 \n",
    "            / NULLIF(COUNT(DISTINCT v.visit_id), 0), 2\n",
    "        ) AS follow_up_ratio,\n",
    "\n",
    "        COUNT(t.treatment_id) AS total_treatments,\n",
    "        SUM(t.treatment_cost) AS total_treatment_cost,\n",
    "        ROUND(AVG(t.treatment_cost), 2) AS avg_treatment_cost\n",
    "\n",
    "    FROM healthcare.gold.dim_doctors d\n",
    "    LEFT JOIN healthcare.gold.dim_visits v ON d.doctor_id = v.doctor_id\n",
    "    LEFT JOIN healthcare.gold.fact_treatments t ON v.visit_id = t.visit_id\n",
    "\n",
    "    GROUP BY \n",
    "        d.doctor_id,\n",
    "        d.doctor_name,\n",
    "        d.department\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------- Execute SQL -------------------------\n",
    "    logger.info(\"üìê Executing SQL to create or replace view: healthcare.gold.vw_doctor_performance\")\n",
    "    spark.sql(view_sql)\n",
    "\n",
    "    # ------------------------- Preview View -------------------------\n",
    "    logger.info(\"üîç Previewing data from vw_doctor_performance\")\n",
    "    preview_df = spark.sql(\"SELECT * FROM healthcare.gold.view_doctor_performance\")\n",
    "    display(preview_df)\n",
    "\n",
    "    logger.info(\"‚úÖ Gold View created successfully: vw_doctor_performance\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8268039575300373,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "doctors view",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
