{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfed34d-8139-49c5-a1ae-fca922940df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da902575-def6-4c49-b896-f9d92a0b50c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "gold_schema = pipeline_config[\"schemas\"][\"gold\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeea464a-0cfc-45aa-9ae8-47b7654c0074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, year, month, dayofweek, weekofyear, quarter, date_add\n",
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize logger using configuration from startup\n",
    "dim_date_logger = create_logger(notebook_name=\"dim_date\", log_level=\"DEBUG\")\n",
    "dim_date_logger.info(\"🚀 Initializing dim_date notebook\")\n",
    "\n",
    "dim_date_logger.info(f\"Configuration initialized with catalog: {catalog}, schema: {gold_schema}, format: {gold_format}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Utility Functions\n",
    "@log_execution(dim_date_logger)\n",
    "def get_table_config(table_name, config):\n",
    "    \"\"\"Retrieve table configuration from provided config dictionary.\"\"\"\n",
    "    dim_date_logger.debug(f\"Retrieving configuration for table: {table_name}\")\n",
    "    return config.get(table_name, {})\n",
    "\n",
    "@log_execution(dim_date_logger)\n",
    "def get_spark_types(type_str):\n",
    "    \"\"\"Map config types to Spark data types.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"INT\": IntegerType(),\n",
    "        \"STRING\": StringType(),\n",
    "        \"BOOLEAN\": BooleanType(),\n",
    "        \"DATE\": DateType()\n",
    "    }\n",
    "    dim_date_logger.debug(f\"Mapping type '{type_str}' to Spark type\")\n",
    "    return type_mapping.get(type_str.upper())\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Date Dimension Generator\n",
    "@log_execution(dim_date_logger)\n",
    "def generate_date_dim(spark, start_date, end_date):\n",
    "    \"\"\"Generate date dimension DataFrame.\"\"\"\n",
    "    dim_date_logger.log_start(f\"generating date dimension from {start_date} to {end_date}\", \"transformation\")\n",
    "\n",
    "    try:\n",
    "        # Validate date formats\n",
    "        def is_valid_date(date_str):\n",
    "            try:\n",
    "                datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "\n",
    "        if not is_valid_date(start_date):\n",
    "            dim_date_logger.error(f\"Invalid start_date format: {start_date}. Expected format: yyyy-MM-dd\")\n",
    "            raise ValueError(f\"Invalid start_date format: {start_date}. Expected format: yyyy-MM-dd\")\n",
    "        if not is_valid_date(end_date):\n",
    "            dim_date_logger.error(f\"Invalid end_date format: {end_date}. Expected format: yyyy-MM-dd\")\n",
    "            raise ValueError(f\"Invalid end_date format: {end_date}. Expected format: yyyy-MM-dd\")\n",
    "\n",
    "        dim_date_logger.debug(f\"Validated start_date: {start_date}, end_date: {end_date}\")\n",
    "\n",
    "        # Create a DataFrame with the date range parameters\n",
    "        dates_df = spark.createDataFrame([(start_date, end_date)], [\"start_date\", \"end_date\"])\n",
    "        dates_df.createOrReplaceTempView(\"dates_temp\")\n",
    "\n",
    "        # Use the DataFrame columns in the SQL query to avoid string injection\n",
    "        df = spark.sql(f\"\"\"\n",
    "            WITH dates AS (\n",
    "                SELECT\n",
    "                    TO_DATE(start_date) AS start_date,\n",
    "                    TO_DATE(end_date) AS end_date\n",
    "                FROM dates_temp\n",
    "            ),\n",
    "            numbers AS (\n",
    "                SELECT EXPLODE(SEQUENCE(0, DATEDIFF(end_date, start_date))) AS num\n",
    "                FROM dates\n",
    "            ),\n",
    "            date_range AS (\n",
    "                SELECT DATE_ADD((SELECT start_date FROM dates), num) AS full_date\n",
    "                FROM numbers\n",
    "            )\n",
    "            SELECT\n",
    "                CAST(DATE_FORMAT(full_date, 'yyyyMMdd') AS INT) AS date_id,\n",
    "                DATE_FORMAT(full_date, 'EEEE') AS day_name,\n",
    "                DATE_FORMAT(full_date, 'MMMM') AS month_name,\n",
    "                CAST(\n",
    "                    CASE\n",
    "                        WHEN DAYOFWEEK(full_date) = 1 THEN 7\n",
    "                        ELSE DAYOFWEEK(full_date) - 1\n",
    "                    END AS INT\n",
    "                ) AS day_of_week,\n",
    "                CAST(WEEKOFYEAR(full_date) AS INT) AS week_of_year,\n",
    "                CAST(MONTH(full_date) AS INT) AS month_number,\n",
    "                CONCAT('Q', CAST(QUARTER(full_date) AS STRING)) AS quarter,\n",
    "                CAST(YEAR(full_date) AS INT) AS calendar_year,\n",
    "                CAST(\n",
    "                    CASE\n",
    "                        WHEN MONTH(full_date) >= 4 THEN YEAR(full_date) + 1\n",
    "                        ELSE YEAR(full_date)\n",
    "                    END AS INT\n",
    "                ) AS fiscal_year,\n",
    "                CAST(\n",
    "                    CASE WHEN DAYOFWEEK(full_date) IN (1, 7) THEN TRUE ELSE FALSE END\n",
    "                    AS BOOLEAN\n",
    "                ) AS is_weekend,\n",
    "                full_date\n",
    "            FROM date_range\n",
    "            ORDER BY date_id\n",
    "        \"\"\")\n",
    "\n",
    "        # Log DataFrame information\n",
    "        log_dataframe_info(df, \"date_dimension\", dim_date_logger, \"transformation\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        dim_date_logger.error(f\"Error generating date dimension: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Table Creation\n",
    "@log_execution(dim_date_logger)\n",
    "def create_table(spark, table_name, table_config, storage_path):\n",
    "    \"\"\"Create Delta table based on config.\"\"\"\n",
    "    dim_date_logger.log_start(f\"creating table {table_name}\", \"data_writing\")\n",
    "\n",
    "    try:\n",
    "        schema_def = \", \".join(\n",
    "            f\"{col['name']} {col['type']}{' NOT NULL' if not col.get('nullable', True) else ''}\"\n",
    "            for col in table_config[\"schema\"]\n",
    "        )\n",
    "        primary_key = table_config.get(\"primary_key\")\n",
    "        partition_by = table_config.get(\"partition_by\")\n",
    "\n",
    "        dim_date_logger.info(f\"Creating table with primary key: {primary_key}, partitioned by: {partition_by}\")\n",
    "\n",
    "        create_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                {schema_def},\n",
    "                CONSTRAINT pk_{primary_key} PRIMARY KEY ({primary_key})\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY ({partition_by})\n",
    "        \"\"\"\n",
    "\n",
    "        dim_date_logger.debug(f\"Executing CREATE TABLE SQL: {create_sql}\")\n",
    "        spark.sql(create_sql)\n",
    "\n",
    "        dim_date_logger.info(f\"Successfully created table {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        dim_date_logger.error(f\"Error creating table {table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Main Processing Function\n",
    "@log_execution(dim_date_logger)\n",
    "def process_dim_date(spark, catalog, schema, table_config, storage_path, start_date=\"2000-01-01\"):\n",
    "    \"\"\"Main function to process dim_date table.\"\"\"\n",
    "    dim_date_logger.log_start(f\"processing dim_date table\", \"transformation\")\n",
    "\n",
    "    table_name = f\"{catalog}.{schema}.dim_date\"\n",
    "    dim_date_logger.info(f\"Processing table: {table_name}\")\n",
    "    dim_date_logger.info(f\"Storage path: {storage_path}\")\n",
    "    dim_date_logger.info(f\"Start date: {start_date}\")\n",
    "\n",
    "    try:\n",
    "        # Get current date and year\n",
    "        dim_date_logger.debug(\"Getting current date and year\")\n",
    "        current_date_df = spark.sql(\"SELECT CURRENT_DATE() AS current_date\")\n",
    "        current_date = current_date_df.collect()[0][\"current_date\"]\n",
    "        current_year = spark.sql(\"SELECT YEAR(CURRENT_DATE()) AS current_year\").collect()[0][\"current_year\"]\n",
    "\n",
    "        dim_date_logger.info(f\"Current date: {current_date}, Current year: {current_year}\")\n",
    "\n",
    "        # Check if table exists\n",
    "        dim_date_logger.debug(f\"Checking if table {table_name} exists\")\n",
    "        table_exists = spark.catalog.tableExists(table_name)\n",
    "        dim_date_logger.info(f\"Table exists: {table_exists}\")\n",
    "\n",
    "        if not table_exists:\n",
    "            # Create table and populate with initial data\n",
    "            dim_date_logger.info(\"Table does not exist, creating new table with initial data\")\n",
    "            end_date = f\"{current_year}-12-31\"\n",
    "            dim_date_logger.info(f\"Generating initial data from {start_date} to {end_date}\")\n",
    "\n",
    "            df = generate_date_dim(spark, start_date, end_date)\n",
    "            create_table(spark, table_name, table_config, storage_path)\n",
    "\n",
    "            dim_date_logger.log_start(f\"writing initial data to {table_name}\", \"data_writing\")\n",
    "            df.write.format(\"delta\").partitionBy(table_config[\"partition_by\"]).mode(\"append\").saveAsTable(table_name)\n",
    "            dim_date_logger.info(f\"Created table {table_name} with data from {start_date} to {end_date}\")\n",
    "\n",
    "        else:\n",
    "            # Check if current date exists\n",
    "            dim_date_logger.debug(\"Table exists, checking if current date data exists\")\n",
    "            current_date_exists = spark.sql(f\"\"\"\n",
    "                SELECT COUNT(*) AS cnt\n",
    "                FROM {table_name}\n",
    "                WHERE full_date = CURRENT_DATE()\n",
    "            \"\"\").collect()[0][\"cnt\"] > 0\n",
    "\n",
    "            dim_date_logger.info(f\"Current date exists in table: {current_date_exists}\")\n",
    "\n",
    "            if not current_date_exists:\n",
    "                # Append data for current year\n",
    "                dim_date_logger.info(\"Current date not found, appending data for current year\")\n",
    "                start_of_year = f\"{current_year}-01-01\"\n",
    "                end_of_year = f\"{current_year}-12-31\"\n",
    "                dim_date_logger.info(f\"Generating data for year {current_year} from {start_of_year} to {end_of_year}\")\n",
    "\n",
    "                df = generate_date_dim(spark, start_of_year, end_of_year)\n",
    "\n",
    "                dim_date_logger.log_start(f\"appending year {current_year} data to {table_name}\", \"data_writing\")\n",
    "                df.write.format(\"delta\").partitionBy(table_config[\"partition_by\"]).mode(\"append\").saveAsTable(table_name)\n",
    "                dim_date_logger.info(f\"Appended data for year {current_year} to {table_name}\")\n",
    "\n",
    "            else:\n",
    "                dim_date_logger.info(f\"Current date {current_date} already exists in {table_name}. No action needed.\")\n",
    "\n",
    "        # Optimize table\n",
    "        dim_date_logger.log_start(f\"optimizing table {table_name}\", \"data_writing\")\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "        dim_date_logger.info(f\"Successfully optimized table {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        dim_date_logger.error(f\"Error processing dim_date table: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Main execution\n",
    "dim_date_logger.info(\"Starting dim_date processing\")\n",
    "\n",
    "# Process dim_date table using configuration\n",
    "try:\n",
    "    process_dim_date(spark, catalog, gold_schema, table_config[\"dim_date\"], storage_path)\n",
    "    dim_date_logger.info(\"✅ Completed dim_date processing successfully\")\n",
    "except Exception as e:\n",
    "    dim_date_logger.error(f\"❌ Failed to process dim_date: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dim_date",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
