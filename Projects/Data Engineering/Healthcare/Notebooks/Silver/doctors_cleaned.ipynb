{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9099960e-4590-47ee-9861-2de167735712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421b7924-f2eb-4d0e-a1ec-1a17667ee2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = create_logger(notebook_name=\"landing_to_bronze\", log_level=\"DEBUG\")\n",
    "logger.info(\"🚀 Initializing landing_to_bronze notebook\")\n",
    "\n",
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "bronze_schema = pipeline_config[\"schemas\"][\"bronze\"]\n",
    "bronze_path = pipeline_config[\"paths\"][\"bronze_path\"]\n",
    "silver_schema = pipeline_config[\"schemas\"][\"silver\"]\n",
    "silver_path = pipeline_config[\"paths\"][\"silver_path\"]\n",
    "landing_schema= pipeline_config[\"schemas\"][\"landing\"]\n",
    "landing_path = pipeline_config[\"paths\"][\"landing_path\"]\n",
    "logs_schema = pipeline_config[\"schemas\"][\"logs\"]\n",
    "logger.info(\"Extracted frequently used config values into variables\")\n",
    "table_name=\"doctors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c908a23-7a11-4b04-90f5-f2f81eed3bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver_errors\")\n",
    "\n",
    "# Paths and targets\n",
    "bronze_input_path = f\"{bronze_volume_path}/{table_name}\"\n",
    "silver_output_table = f\"{catalog}.{silver_schema}.{table_name}\"\n",
    "error_output_table = f\"{catalog}.silver_errors.{table_name}_errors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25c158f-792e-491d-af23-329939a75ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# doctors_to_silver.py (SCD Type 2 - Final: Accurate Change Detection)\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, trim, upper, monotonically_increasing_id, when\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Setup ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver_errors\")\n",
    "\n",
    "# Paths and targets\n",
    "bronze_input_path = f\"{bronze_volume_path}/{table_name}\"\n",
    "silver_output_table = f\"{catalog}.{silver_schema}.{table_name}\"\n",
    "error_output_table = f\"{catalog}.silver_errors.{table_name}_errors\"\n",
    "\n",
    "# COMMAND ----------\n",
    "def read_bronze_csv(path):\n",
    "    logger.info(f\"📥 Reading from Bronze: {path}\")\n",
    "    return spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "def apply_column_mapping(df):\n",
    "    logger.info(\"🔀 Applying column mappings\")\n",
    "    for old_col, new_col in column_mappings[table_name].items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "def enforce_data_types(df):\n",
    "    logger.info(\"🧪 Enforcing data types\")\n",
    "    for col_name, dtype in table_config[table_name][\"columns\"].items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(dtype))\n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "def run_dq_checks(df):\n",
    "    logger.info(\"✅ Running DQ checks for 'doctors'\")\n",
    "    dq_errors = None\n",
    "\n",
    "    def append_dq(df_err, new_rows, reason):\n",
    "        new_rows = new_rows.withColumn(\"dq_error\", lit(reason))\n",
    "        return new_rows if df_err is None else df_err.unionByName(new_rows)\n",
    "\n",
    "    if \"doctor_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"doctor_id\").isNull()), \"doctor_id is null\")\n",
    "\n",
    "    if \"doctor_name\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(trim(col(\"doctor_name\")) == \"\"), \"doctor_name is blank\")\n",
    "\n",
    "    if \"experience_years\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"experience_years\") < 0), \"experience_years is negative\")\n",
    "\n",
    "    if \"department\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"department\").isNull()), \"department is null\")\n",
    "\n",
    "    return dq_errors\n",
    "\n",
    "# COMMAND ----------\n",
    "def filter_and_store_errors(df, dq_errors):\n",
    "    logger.info(\"🗑️ Filtering bad records\")\n",
    "    valid_df = df\n",
    "\n",
    "    if dq_errors is not None:\n",
    "        dq_errors = dq_errors.withColumn(\"dq_error_id\", monotonically_increasing_id())\n",
    "        dq_errors = dq_errors.withColumn(\"error_ts\", current_timestamp())\n",
    "        dq_errors.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(error_output_table)\n",
    "        logger.info(f\"⚠️ {dq_errors.count()} DQ errors written to: {error_output_table}\")\n",
    "\n",
    "        valid_df = df.join(dq_errors.select(\"doctor_id\").distinct(), on=\"doctor_id\", how=\"left_anti\")\n",
    "\n",
    "    return valid_df\n",
    "\n",
    "# COMMAND ----------\n",
    "def apply_scd_type2(df):\n",
    "    logger.info(f\"🔁 Applying SCD Type 2 to: {silver_output_table}\")\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_output_table):\n",
    "        df = df.withColumn(\"valid_from\", current_timestamp()) \\\n",
    "               .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "               .withColumn(\"is_current\", lit(True))\n",
    "        df.write.format(\"delta\").saveAsTable(silver_output_table)\n",
    "        logger.info(f\"✅ Created new SCD Type 2 table: {silver_output_table}\")\n",
    "        return\n",
    "\n",
    "    current_df = spark.table(silver_output_table).filter(\"is_current = true\")\n",
    "    join_keys = [\"doctor_id\"]\n",
    "    compare_cols = [c for c in df.columns if c not in join_keys + [\"valid_from\", \"valid_to\", \"is_current\", \"ingestion_date\"]]\n",
    "\n",
    "    source_df = df.alias(\"source\")\n",
    "    target_df = current_df.alias(\"target\")\n",
    "\n",
    "    join_cond = \" AND \".join([f\"source.{k} = target.{k}\" for k in join_keys])\n",
    "    change_expr = \" OR \".join([f\"target.{c} IS DISTINCT FROM source.{c}\" for c in compare_cols])\n",
    "\n",
    "    joined_df = source_df.join(target_df, on=join_keys, how=\"left\")\n",
    "    changed_df = joined_df.filter(f\"{change_expr} OR target.{join_keys[0]} IS NULL\").select(\"source.*\")\n",
    "\n",
    "    logger.info(f\"📌 Changed/new rows: {changed_df.count()}\")\n",
    "\n",
    "    if changed_df.count() == 0:\n",
    "        logger.info(\"✅ No changes found. Skipping SCD2 merge.\")\n",
    "        return\n",
    "\n",
    "    changed_df = changed_df \\\n",
    "        .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, silver_output_table)\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        source=changed_df.alias(\"source\"),\n",
    "        condition=join_cond + \" AND target.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=change_expr,\n",
    "        set={\n",
    "            \"valid_to\": \"current_timestamp()\",\n",
    "            \"is_current\": \"false\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    changed_df = changed_df.select(\n",
    "        \"doctor_id\",\n",
    "        \"doctor_name\",\n",
    "        \"department\",\n",
    "        \"experience_years\",\n",
    "        \"ingestion_date\",\n",
    "        \"valid_from\",\n",
    "        \"valid_to\",\n",
    "        \"is_current\"\n",
    "    )\n",
    "\n",
    "    changed_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(silver_output_table)\n",
    "\n",
    "    logger.info(\"✅ SCD2 merge completed: history updated and new rows inserted.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Execute full pipeline\n",
    "bronze_df = read_bronze_csv(bronze_input_path)\n",
    "bronze_df = apply_column_mapping(bronze_df)\n",
    "bronze_df = enforce_data_types(bronze_df)\n",
    "if \"ingestion_date\" not in bronze_df.columns:\n",
    "    bronze_df = bronze_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "dq_errors = run_dq_checks(bronze_df)\n",
    "clean_df = filter_and_store_errors(bronze_df, dq_errors)\n",
    "\n",
    "apply_scd_type2(clean_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4531891376042004,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "doctors_cleaned",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
