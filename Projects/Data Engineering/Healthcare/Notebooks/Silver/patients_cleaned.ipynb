{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ccc7be-abb2-42ef-a193-60acf3a8111e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb855ca-4c00-4abf-94e7-3ea1c77de7cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = create_logger(notebook_name=\"landing_to_bronze\", log_level=\"DEBUG\")\n",
    "logger.info(\"üöÄ Initializing landing_to_bronze notebook\")\n",
    "\n",
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "bronze_schema = pipeline_config[\"schemas\"][\"bronze\"]\n",
    "bronze_path = pipeline_config[\"paths\"][\"bronze_path\"]\n",
    "bronze_volume_path = pipeline_config[\"paths\"][\"bronze_volume_path\"]\n",
    "silver_schema = pipeline_config[\"schemas\"][\"silver\"]\n",
    "silver_path = pipeline_config[\"paths\"][\"silver_path\"]\n",
    "landing_schema= pipeline_config[\"schemas\"][\"landing\"]\n",
    "landing_path = pipeline_config[\"paths\"][\"landing_path\"]\n",
    "logs_schema = pipeline_config[\"schemas\"][\"logs\"]\n",
    "table_name = \"patients\"\n",
    "logger.info(\"Extracted frequently used config values into variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b7391c-4364-4c5a-9bb3-b80c872bbedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# patients_to_silver.py (SCD Type 2 - Corrected: Add valid_from only for new/changed)\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, trim, upper, monotonically_increasing_id, when\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Setup ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver_errors\")\n",
    "\n",
    "# Paths and targets\n",
    "bronze_input_path = f\"{bronze_volume_path}/{table_name}\"\n",
    "silver_output_table = f\"{catalog}.{silver_schema}.{table_name}\"\n",
    "error_output_table = f\"{catalog}.silver_errors.{table_name}_errors\"\n",
    "\n",
    "# COMMAND ----------\n",
    "def read_bronze_csv(path):\n",
    "    logger.info(f\"üì• Reading from Bronze: {path}\")\n",
    "    return spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "def apply_column_mapping(df):\n",
    "    logger.info(\"üîÄ Applying column mappings\")\n",
    "    for old_col, new_col in column_mappings[table_name].items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "def enforce_data_types(df):\n",
    "    logger.info(\"üß™ Enforcing data types\")\n",
    "    for col_name, dtype in table_config[table_name][\"columns\"].items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(dtype))\n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "def standardize_values(df):\n",
    "    logger.info(\"‚ôªÔ∏è Standardizing gender values\")\n",
    "    if \"gender\" in df.columns:\n",
    "        df = df.withColumn(\"gender\",\n",
    "            when(upper(col(\"gender\")) == \"MALE\", \"M\")\n",
    "           .when(upper(col(\"gender\")) == \"FEMALE\", \"F\")\n",
    "           .when(upper(col(\"gender\")) == \"OTHER\", \"O\")\n",
    "           .otherwise(col(\"gender\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "def run_dq_checks(df):\n",
    "    logger.info(\"‚úÖ Running DQ checks for 'patients'\")\n",
    "    dq_errors = None\n",
    "\n",
    "    def append_dq(df_err, new_rows, reason):\n",
    "        new_rows = new_rows.withColumn(\"dq_error\", lit(reason))\n",
    "        return new_rows if df_err is None else df_err.unionByName(new_rows)\n",
    "\n",
    "    if \"patient_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"patient_id\").isNull()), \"patient_id is null\")\n",
    "\n",
    "    if \"patient_name\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(trim(col(\"patient_name\")) == \"\"), \"patient_name is blank\")\n",
    "\n",
    "    if \"sex\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"sex\") < 0), \"sex (age) is negative\")\n",
    "\n",
    "    if \"gender\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(~upper(col(\"gender\")).isin(\"M\", \"F\", \"O\")), \"gender is not M/F/O\")\n",
    "\n",
    "    if \"country\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"country\").isNull()), \"country is null\")\n",
    "\n",
    "    return dq_errors\n",
    "\n",
    "# COMMAND ----------\n",
    "def filter_and_store_errors(df, dq_errors):\n",
    "    logger.info(\"üóëÔ∏è Filtering bad records\")\n",
    "    valid_df = df\n",
    "\n",
    "    if dq_errors is not None:\n",
    "        dq_errors = dq_errors.withColumn(\"dq_error_id\", monotonically_increasing_id())\n",
    "        dq_errors = dq_errors.withColumn(\"error_ts\", current_timestamp())\n",
    "        dq_errors.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(error_output_table)\n",
    "        logger.info(f\"‚ö†Ô∏è {dq_errors.count()} DQ errors written to: {error_output_table}\")\n",
    "\n",
    "        valid_df = df.join(dq_errors.select(\"patient_id\").distinct(), on=\"patient_id\", how=\"left_anti\")\n",
    "\n",
    "    return valid_df\n",
    "\n",
    "# COMMAND ----------\n",
    "def apply_scd_type2(df):\n",
    "    logger.info(f\"üîÅ Applying SCD Type 2 using PySpark to: {silver_output_table}\")\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_output_table):\n",
    "        df = df.withColumn(\"valid_from\", current_timestamp()) \\\n",
    "               .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "               .withColumn(\"is_current\", lit(True))\n",
    "        df.write.format(\"delta\").saveAsTable(silver_output_table)\n",
    "        logger.info(f\"‚úÖ Created new SCD Type 2 table: {silver_output_table}\")\n",
    "        return\n",
    "\n",
    "    # Load current data only\n",
    "    target_df = spark.table(silver_output_table).filter(\"is_current = true\")\n",
    "\n",
    "    join_keys = [\"patient_id\"]\n",
    "    compare_cols = [c for c in df.columns if c not in join_keys]\n",
    "\n",
    "    joined = df.alias(\"source\").join(target_df.alias(\"target\"), on=join_keys, how=\"left\")\n",
    "    changed_records = joined.filter(\n",
    "        \" OR \".join([f\"source.{c} IS DISTINCT FROM target.{c}\" for c in compare_cols]) +\n",
    "        f\" OR target.{join_keys[0]} IS NULL\"\n",
    "    ).select(\"source.*\")\n",
    "\n",
    "    logger.info(f\"üìå Total changed/new records to upsert: {changed_records.count()}\")\n",
    "\n",
    "    if changed_records.count() == 0:\n",
    "        logger.info(\"‚úÖ No changes detected. Skipping merge.\")\n",
    "        return\n",
    "\n",
    "    changed_records = changed_records \\\n",
    "        .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, silver_output_table)\n",
    "    cond = \" AND \".join([f\"target.{key} = source.{key}\" for key in join_keys]) + \" AND target.is_current = true\"\n",
    "\n",
    "    update_expr = {\"valid_to\": \"current_timestamp()\", \"is_current\": \"false\"}\n",
    "    insert_expr = {col: f\"source.{col}\" for col in changed_records.columns}\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        source=changed_records.alias(\"source\"),\n",
    "        condition=cond\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\" OR \".join([\n",
    "            f\"target.{c} IS DISTINCT FROM source.{c}\"\n",
    "            for c in compare_cols\n",
    "        ]),\n",
    "        set=update_expr\n",
    "    ).whenNotMatchedInsert(\n",
    "        values=insert_expr\n",
    "    ).execute()\n",
    "\n",
    "    logger.info(f\"‚úÖ SCD Type 2 merge completed for updated/new records only.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Execute full pipeline\n",
    "bronze_df = read_bronze_csv(bronze_input_path)\n",
    "bronze_df = apply_column_mapping(bronze_df)\n",
    "bronze_df = standardize_values(bronze_df)\n",
    "bronze_df = enforce_data_types(bronze_df)\n",
    "if \"ingestion_date\" not in bronze_df.columns:\n",
    "    bronze_df = bronze_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "dq_errors = run_dq_checks(bronze_df)\n",
    "clean_df = filter_and_store_errors(bronze_df, dq_errors)\n",
    "\n",
    "apply_scd_type2(clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334db13c-971a-412f-b073-aeb0922f4c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from healthcare.silver.patients\n",
    "where patient_id='P1000'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7553195272685536,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "patients_cleaned",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
