{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720d496d-6f22-4878-aa68-8a16ba41fef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../.././start_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78415bd5-22d7-4d61-a2c8-c6978b7fc9ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = create_logger(notebook_name=\"landing_to_bronze\", log_level=\"DEBUG\")\n",
    "logger.info(\"üöÄ Initializing landing_to_bronze notebook\")\n",
    "\n",
    "# Extract frequently used config values into variables\n",
    "catalog = pipeline_config[\"catalog\"]\n",
    "bronze_schema = pipeline_config[\"schemas\"][\"bronze\"]\n",
    "bronze_path = pipeline_config[\"paths\"][\"bronze_path\"]\n",
    "bronze_volume_path = pipeline_config[\"paths\"][\"bronze_volume_path\"]\n",
    "silver_schema = pipeline_config[\"schemas\"][\"silver\"]\n",
    "silver_path = pipeline_config[\"paths\"][\"silver_path\"]\n",
    "landing_schema= pipeline_config[\"schemas\"][\"landing\"]\n",
    "landing_path = pipeline_config[\"paths\"][\"landing_path\"]\n",
    "logs_schema = pipeline_config[\"schemas\"][\"logs\"]\n",
    "table_name = \"treatments\"\n",
    "logger.info(\"Extracted frequently used config values into variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd6d9b7-2607-4dbb-b0be-da25375ae12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.silver_errors\")\n",
    "\n",
    "# Paths and targets\n",
    "bronze_input_path = f\"{bronze_volume_path}/{table_name}\"\n",
    "silver_output_table = f\"{catalog}.{silver_schema}.{table_name}\"\n",
    "error_output_table = f\"{catalog}.silver_errors.{table_name}_errors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a407a4cd-369f-4a1d-8f33-d937f6091b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# treatments_to_silver.py (SCD Type 2 - Final: Accurate Change Detection)\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, trim, upper, monotonically_increasing_id, when\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "# Read CSV\n",
    "def read_bronze_csv(path):\n",
    "    logger.info(f\"üì• Reading from Bronze: {path}\")\n",
    "    return spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "# Rename Columns\n",
    "def apply_column_mapping(df):\n",
    "    logger.info(\"üîÄ Applying column mappings\")\n",
    "    for old_col, new_col in column_mappings[table_name].items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "# Enforce Schema\n",
    "def enforce_data_types(df):\n",
    "    logger.info(\"üß™ Enforcing data types\")\n",
    "    for col_name, dtype in table_config[table_name][\"columns\"].items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(dtype))\n",
    "    return df\n",
    "\n",
    "# DQ Checks for treatments\n",
    "def run_dq_checks(df):\n",
    "    logger.info(\"‚úÖ Running DQ checks for 'treatments'\")\n",
    "    dq_errors = None\n",
    "\n",
    "    def append_dq(df_err, new_rows, reason):\n",
    "        new_rows = new_rows.withColumn(\"dq_error\", lit(reason))\n",
    "        return new_rows if df_err is None else df_err.unionByName(new_rows)\n",
    "\n",
    "    if \"treatment_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"treatment_id\").isNull()), \"treatment_id is null\")\n",
    "\n",
    "    if \"patient_id\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(col(\"patient_id\").isNull()), \"patient_id is null\")\n",
    "\n",
    "    if \"treatment_type\" in df.columns:\n",
    "        dq_errors = append_dq(dq_errors, df.filter(trim(col(\"treatment_type\")) == \"\"), \"treatment_type is blank\")\n",
    "\n",
    "    return dq_errors\n",
    "\n",
    "# Filter and write errors\n",
    "def filter_and_store_errors(df, dq_errors):\n",
    "    logger.info(\"üóëÔ∏è Filtering bad records\")\n",
    "    valid_df = df\n",
    "\n",
    "    if dq_errors is not None:\n",
    "        dq_errors = dq_errors.withColumn(\"dq_error_id\", monotonically_increasing_id())\n",
    "        dq_errors = dq_errors.withColumn(\"error_ts\", current_timestamp())\n",
    "        dq_errors.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(error_output_table)\n",
    "        logger.info(f\"‚ö†Ô∏è {dq_errors.count()} DQ errors written to: {error_output_table}\")\n",
    "\n",
    "        valid_df = df.join(dq_errors.select(\"treatment_id\").distinct(), on=\"treatment_id\", how=\"left_anti\")\n",
    "\n",
    "    return valid_df\n",
    "\n",
    "# Apply SCD Type 2\n",
    "def apply_scd_type2(df):\n",
    "    logger.info(f\"üîÅ Applying SCD Type 2 to: {silver_output_table}\")\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_output_table):\n",
    "        df = df.withColumn(\"valid_from\", current_timestamp()) \\\n",
    "               .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "               .withColumn(\"is_current\", lit(True))\n",
    "        df.write.format(\"delta\").saveAsTable(silver_output_table)\n",
    "        logger.info(f\"‚úÖ Created new SCD Type 2 table: {silver_output_table}\")\n",
    "        return\n",
    "\n",
    "    current_df = spark.table(silver_output_table).filter(\"is_current = true\")\n",
    "    join_keys = [\"treatment_id\"]\n",
    "    compare_cols = [c for c in df.columns if c not in join_keys + [\"valid_from\", \"valid_to\", \"is_current\", \"ingestion_date\"]]\n",
    "\n",
    "    source_df = df.alias(\"source\")\n",
    "    target_df = current_df.alias(\"target\")\n",
    "\n",
    "    join_cond = \" AND \".join([f\"source.{k} = target.{k}\" for k in join_keys])\n",
    "    change_expr = \" OR \".join([f\"target.{c} IS DISTINCT FROM source.{c}\" for c in compare_cols])\n",
    "\n",
    "    joined_df = source_df.join(target_df, on=join_keys, how=\"left\")\n",
    "    changed_df = joined_df.filter(f\"{change_expr} OR target.{join_keys[0]} IS NULL\").select(\"source.*\")\n",
    "\n",
    "    logger.info(f\"üìå Changed/new rows: {changed_df.count()}\")\n",
    "\n",
    "    if changed_df.count() == 0:\n",
    "        logger.info(\"‚úÖ No changes found. Skipping SCD2 merge.\")\n",
    "        return\n",
    "\n",
    "    changed_df = changed_df \\\n",
    "        .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, silver_output_table)\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        source=changed_df.alias(\"source\"),\n",
    "        condition=join_cond + \" AND target.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=change_expr,\n",
    "        set={\n",
    "            \"valid_to\": \"current_timestamp()\",\n",
    "            \"is_current\": \"false\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    changed_df = changed_df.select(\n",
    "        \"treatment_id\", \"patient_id\", \"treatment_type\", \"description\", \"ingestion_date\",\n",
    "        \"valid_from\", \"valid_to\", \"is_current\"\n",
    "    )\n",
    "\n",
    "    changed_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(silver_output_table)\n",
    "    logger.info(\"‚úÖ SCD2 merge completed: history updated and new rows inserted.\")\n",
    "\n",
    "# Execute pipeline\n",
    "bronze_df = read_bronze_csv(bronze_input_path)\n",
    "bronze_df = apply_column_mapping(bronze_df)\n",
    "bronze_df = enforce_data_types(bronze_df)\n",
    "if \"ingestion_date\" not in bronze_df.columns:\n",
    "    bronze_df = bronze_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "dq_errors = run_dq_checks(bronze_df)\n",
    "clean_df = filter_and_store_errors(bronze_df, dq_errors)\n",
    "apply_scd_type2(clean_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "treatments_cleaned",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
